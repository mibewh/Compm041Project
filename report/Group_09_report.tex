\documentclass{sig-alternate-05-2015}
\makeatletter
\def\@copyrightspace{\relax}
\makeatother
\begin{document}

% Copyright
\setcopyright{acmcopyright}

\title{Web Economics: Group 09 Report}

\numberofauthors{3}
\author{
\alignauthor
Devin Kuokka\\
       \affaddr{Affiliate Student, Computer Science, University College London}\\
       \email{devin.kuokka.16@ucl.ac.uk}
\alignauthor
Stylianos Rousoglou\\
       \affaddr{Affiliate Student, Computer Science, University College London}\\
       \email{stylianos.rousoglou.16
       @ucl.ac.uk}
\alignauthor
Michael Whitman\\
       \affaddr{Affiliate Student, Computer Science, University College London}\\
       \email{zcabwhi@ucl.ac.uk}
}
\maketitle

\section{Introduction}
Real-Time Bidding (RTB) is an increasingly popular approach to online advertising that has evolved into a multi-billion dollar industry. An efficient approach to quickly and accurately optimizing an advertiser's bidding strategy in ad auctions is paramount to the success of the advertising party, as a good solution can save vast amounts of money and lead to higher conversion rates than competitors.

The challenge in automating ad auctions and bidding in real-time is the necessity to predict, with the highest possible accuracy, the likelihood that the ads displayed to a particular user will actually be of interest to them. Using those predictions, hereby referred to as pCTR (or predicted Click-Through Rate), a strategy can be developed to optimize the number of won impressions (which the advertiser pays for) that have the potential of becoming conversions. Naturally, the accuracy and sensitivity of the pCTR values in central in developing a successful bidding strategy.

Given real-life ad impression data, a machine learning approach is often preferred in tackling this problem. First, a large data set with \textit{training} data is used to build and optimize some type of machine learning classifier. After the machine learning model is trained, it is used to make predictions about data in a \textit{test} data set, for which user feedback is not provided, thus attempting to predict unknown users' behavior. The success of the classifier is evaluated by different metrics that reflect its accuracy in anticipating an unknown user's responsiveness to an ad. The machine learning approach undertaken in my personal strategy is \textit{logistic regression}.

Therefore, in attempting to predict user behavior, a thorough and educated study and use of the training data is required for accurate and useful results. Our individual reports present some data analytics that result from a basic exploration of the training data set provided for the assignment.

In this paper, different approaches to automating the generation of bid prices will be undertaken, and each will be evaluated using common evaluation metrics and comparatively to others. First, two naive strategies with little optimisation will be implemented, and the results will be presented and discussed. Subsequently, two machine learning approaches will be presented, one using a linear model and one using a non-linear one. Our approach and steps in building our machine learning classifier and developing a bidding strategy will be detailed. The techniques employed throughout our group strategies will be discussed, and different design decisions made will be defended. Finally, the results of our best strategy will be presented and commented on using relevant evaluation metrics.

\section{Related Work}
The extensive literature review we performed covers a range of academic papers and articles. The first academic paper, \textit{Real-time bidding benchmarking with ipinyou dataset}, was used as a model for both the presentation of the data and the preliminary statistical analysis performed on the dataset, both to be found in our individual reports. The second relevant publication, \textit{A logistic Regression Approach to Ad Click Prediction}, provided valuable insights into building a logistic regression classifier, which is the machine learning model used in our group solution, as well as into techniques for data pre-processing, data clensing and data reduction. Other related work submitted in the past 5 years to KDD (Knowledge Discovery and Data Mining), a community producing scientific work in data mining and data analysis, was also reviewed for additional observations and expertise.

\section{Dataset}
The dataset consists of three data files, namely the training, validation, and test files. Machine learning classifiers are first trained using the training data set, \textit{train.csv}. Subsequently, the \textit{validation.csv} file is used to evaluate the performance of different classifiers and suggest which algorithm is more accurate in its predictions. While the aforementioned files include user feedback information about each ad impression, which is necessary for supervised learning as it is used to train, optimize and evaluate the classifier (in the case of RTB, user feedback refers to whether the ad was clicked or not, i.e. a binary variable), the third file, \textit{test.csv}, is only used for testing the developed model, and thus does not contain the \textit{click} data field. Auction pricing information is also not included in the testing dataset, as the solution is required to optimize bidding prices and come up with the best possible strategy to win as many likely-to-be-clicked impression auctions as possible, thus maximizing the conversion rate of ad impressions.

\subsection{Data format}
The dataset includes thousands of impressions, one per line, in comma-separated csv files, with detailed information about the advertiser, the advertising context, the spatial and temporal context, and the user, for each impression. The format of the data varies across fields, with integers, strings, as well as special words and symbols used. In instances where data is missing, the word \textit{null} is used. Fields such as \textit{weekday} and \textit{hour} have been mapped to integers, while others, such as \textit{useragent}, have been left as string. It is clear that some data manipulation has to be performed prior to the data beind used for training a classification model.

Note that, although the dataset uses the currency RMB and units of Fen x 1000 in all figures pertaining to money, all monetary results in this report (such as cost, effective CPC, etc.) have been adjusted to units of Fen.

\section{Approach \& Evaluation}
There were several non-trivial approaches our group experimented with, including linear and non-linear solutions. Below, the constant and random bidding strategies are first discussed, and then a linear and a non-linear approach are presented.

Python was used for developing the main functionality of all the solutions and learning algorithms, as well as for common helper code used for loading the data, storing the output in a convenient format, and displaying a summary of the results in the terminal. The \textit{scikit-learn} \footnote{http://scikit-learn.org} Python library was used for the actual logistic regression functionality. Other Python libraries, such as \textit{pandas}\footnote{http://pandas.pydata.org/}, a data analysis software library, and \textit{numpy}\footnote{http://www.numpy.org/}, a scientific computing software package, were also utilized to facilitate data analysis and manipulation.

\subsection{Basic Strategies}

\subsubsection{Constant Bid}

\begin{figure}{t}
  \includegraphics[width=\linewidth]{constant_bids.png}
  \caption{Constant bid vs. Placed bids}
  \label{fig:bids}
\end{figure}

\begin{figure}{t}
  \includegraphics[width=\linewidth]{constant_wins.png}
  \caption{Constant bid vs. Wins}
  \label{fig:wins}
\end{figure}

\begin{figure}{t}
  \includegraphics[width=\linewidth]{constant_clicks.png}
  \caption{Constant bid vs. Clicks}
  \label{fig:clicks}
\end{figure}

\begin{figure}{t}
  \includegraphics[width=\linewidth]{constant_ctr.png}
  \caption{Constant bid vs. CTR}
  \label{fig:CPM}
\end{figure}

The first strategy was a rather simple one; the solution had to bid the same constant value in every auction. Given the budget constraint of $25,000$, the task can be treated as an optimization problem with the following limiting cases: the algorithm's bids are too high, which results to all auctions being won and the budget rapidly running out; or the algorithm's bids are too low, which results to too few auctions being won and the budget not being spent. 

Clearly, the optimal solution lies somewhere in between the two limiting cases. This approach would in theory help us win more bids than choosing a constant bid by chance, and assuming that impressions that will be clicked are uniformly distributed in the dataset, it also increases the expected CTR, since the budget will all be spent, but not immediately and on consecutive bids. To optimize our constant bid value, we repeated the evaluation process for every constant bid value in the range [10, 20, ..., 400]. We then plotted that range of possible values against several evaluation metrics, with the results presented in figures 1-6.

Figures 1 and 2 show the constant bid value plotted against the bids placed and the bids won by the solution respectively. The results are perfectly consistent with the notion that a high constant bid value places less bids and wins less auctions that a lower one. As such, when the solution stops bidding in all auctions (i.e. runs out of money before running out of impressions), both graphs are asymptotically decreasing.

Figure 3 plots the bid value against the number of clicked impressions. The peak of absolute clicks is observed around a constant bid value of $110$, with 130 clicks. The rate at which clicks grow as a function of the bid before that global maximum is much higher than the rate of descent thereafter, because slight increases in the bidding price before the maximum can encapsulate many more won impressions, whereas after the peak, they just decrease the number of total auctions, but not necessarily ones that lead to conversions. 

Figure 4 plots the CTR against the constant bid price. Although the absolute number of clicks is decreasing after the global maximum, the CTR has an increasing trend throughout. This can be attributed to the fact the the clicks curve is decending linearly (in approximation), whereas the wins plot is decending in a higher rate.

\begin{figure}
  \includegraphics[width=\linewidth]{constant_cpm.png}
  \caption{Constant bid vs. CPM}
  \label{fig:CPM}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{constant_cpc.png}
  \caption{Constant bid vs. CPC}
  \label{fig:CPM}
\end{figure}

Figures 5 and 6 plot the CPM and CPC against the constant bid respectively. The average cost per million impressions expectedly increases since higher constant bids implies more expensive auctions being won, and thus more money being spent per million impressions. The average cost per click is initially increasing, before remaining relatively stable for the rest of the test range. This can be explained since as the constant bid increases, so does the number of clicks, but the number of won impressions decreases as a result of more expensive auctions being won. Assuming the clicks are evenly distributed in the dataset, the CPC should indeed not vary  much.

Whereas the CTR is increasing throughout the range, the average cost per million impressions is also increasing, which means that the optimal approach using the constant bid strategy is not necessarily the approach that yields the highest CTR, but rather a tradeoff between the cost and efficiency. Consequently, the CPC may be a better metric to rely upon, since a local minimum in the CPC plot maximizes the efficiency of money in terms of conversions achieved. For instance, a good pick for the constant bid would be the value $230$. The corresponding CTR value is $0.085\%$, which is among the highest percentages in the plot, and the CPC is at a local minimum of $86,200$ per click. Clearly, this solution is not efficient financially, and hopefully the CPC value will drop drastically in latter solutions.

\begin{figure}
  \includegraphics[width=\linewidth]{random_bids.png}
  \caption{Upper bound vs. Placed bids}
  \label{fig:bids}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{random_wins.png}
  \caption{Upper bound vs. Wins}
  \label{fig:wins}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{random_clicks.png}
  \caption{Upper bound vs. Clicks}
  \label{fig:clicks}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{random_ctr.png}
  \caption{Upper bound vs. CTR}
  \label{fig:CPM}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{random_cpm.png}
  \caption{Upper bound vs. CPM}
  \label{fig:CPM}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{random_cpc.png}
  \caption{Upper bound vs. CPC}
  \label{fig:CPM}
\end{figure}

\subsubsection{Random Bid}

The second simple strategy we explored introduced randomness in the strategy's bidding choices by basing the bid prices on chance. Our approach in exploring a range of possible upper bounds was the following: after calculating the mean impression pay price from the training dataset, we created a range of values by adding fractions of the standard deviation to the mean. To make our results more reliable, we ran the evaluation process on validation.csv \textit{five times} for each value in the explored upper bound range. The vertical axes then plot the \textit{average} values obtained from five distinct trials with each given upper bound.

To enforce the upper bounds, we used a Python function to generate pseudo-random numbers in the range $[0,1]$, and then multiplied them by our upper bounds, thus getting a series of pseudo-randomly generated real values in the range $[0, UpperBound]$.

Figures 7 and 8 show the number of placed bids and wins respectively, as a function of the upper bound fed into our pseudo-random number generator. As expected, increasing the upper bound of the random bids leads to a decrease in the expected average number of bids placed. Rougly around the value $200$, the solution starts exhausting the budget by buying more expensive auctions, thus the decending trend in figure 7. At the same value, figure 8 peaks and then start decreasing for the same reason.

Figure 9, which plots the upper bound against the number of clicked impressions, reveals why a random-based approach to solving the problem is bound to not perform reliably well. Although repeating the experiment infinite times would give expected values that follow a consistent trend, even five repetitions lead to (average) values with some variance from an approximate curve like the one observed in figure 3.

Figures 10, 11, 12 look similar to figures 4, 5 and 6 respectively, as the expected results from repeated trials in the random bid solution should closely resemble the results of the constant bid approach, always with some variance due to the degree of randomness involved in the particular bidding approach.

\subsection{Machine Learning Strategies}
Several regression and classification machine learning algorithms were considered by our team. The problem lends itself well for supervised learning; the training dataset does contain auction results as well as user feedback, and the goal is to build a classifier that produce a CTR estimate for any given impression. Our initial instinct was to use Linear Regression, a quite familiar and conceptually simple approach. Linear Regression uses the general linear function $y = a_0 +  \sum a_i x_i$, where the dependent variable $y$ is \textit{continuous}, as is \textit{usually} the case for the independent variables $x_i$ as well. It is tempting to use the linear model result as a probability, but there is a fundamental problem with that: the output of the linear model \textit{can be less than 0 or greater than 1}, rendering the output meaningless.

To solve this problem, \textit{Logistic Regression} was introduced, with a mathematical model that actually restricts the dependent variable value in the range $[0,1]$. In reality, the model is estimating \textit{the probability} of a categorical outcome, i.e. the likelihood of one discrete output result versus another. The general Logistic Regression formula for two outcomes (0 or 1) can be written as \[P(Y=1) = \frac{1}{1+e^-(a_0+\sum a_i x_i)}\]
Though the independent variables may either be continuous or discrete, the result of the model is a probability of the dependent output taking the given discrete value.\footnote{https://www.quora.com/What-is-logistic-regression}

However, the Logistic regression is not the approach that we ended up using. We chose to use a \textit{Random Forest Classifier} as our learning method.\footnote{https://en.wikipedia.org/wiki/Random\_forest} This basically uses a group of decision trees to make its decision, and we found that given the rather unpredictive nature of the data, and the number of categorical features, that this was a model best suited to our pCTR prediction problem.

Before training the machine learning classifiers to make CTR predictions, there was a significant amount of work to be done in order to prepare the data for "learning". First, a high-level analysis (presented in detail in our individual reports) was useful in deciding on the good data features to be used for classification, i.e. the ones that do correlate with differences in the Click-Through Rate. Accordingly, several data fields that either had unique or highly differentiated values, such as the \textit{bidid} and the \textit{userid}, would not have been useful for classification and thus had to be removed. Therefore, some \textbf{data selection} was performed to remove those fields, as well as \textit{IP} (unique to each user), \textit{logtype} (1 in this dataset), \textit{domain}, \textit{url}, \textit{urlid}, \textit{creative}, \textit{keypage}, etc.

Given the large volume of data with negative user feedback (click = 0) and the scarcity of impressions that led to conversions (click = 1), \textbf{undersampling} was also used to increase the ratio of clicked to non-clicked impressions in the training data. The small bias towards clicked impressions in training helped correct the bias in the original dataset, where only 0.077\% of all impressions were clicked. The method of undersampling that we implemented allows us to set a ratio of number of number of nonclicks to number of clicks in our training dataset.

Subsequently, the data was transformed using a technique known as \textit{one-hot encoding}. One-hot encoding creates a vector for every possible string value any feature can possibly take, transforming the feature-value mappings chosen for learning into sparse matrices, One-hot encoding transforms complex feature-value mappings into simple (though very large) matrices with only binary digits, a conversion which is very helpful for mathematical computations as well as more convenient for machine learning classification algorithms.

Next, we take some fields that are in the data and modify them such that they will be more useful features. We take weekday and hour, and replace these with binary features which indicate whether or not the impression occurs during a peak time. We split useragent into two fields, for both operating system and browser used. Using the operating system information, we determine whether or not a mobile device is being used and add that as a feature.

After this last step, the classifier is ready to be trained. The selected feature vectors are passed into the \textit{scikit-learn} library, and a logistic regression model is built to produce CTR predictions. It's worth noting that the option 'class weight' is set to such that clicks are weighted more highly in the classification process than nonclicks, to an amount that we tune depending on the strategy we are implementing.

After learning is completed, the validation dataset is loaded and some data manipulation is performed, as described above. Subsequently, the model is used to calculate CTR predictions for every impression in the validation dataset and store them in an in-memory array. The real-time bidding simulation can then be performed, and its results evaluated.

\begin{figure}
  \includegraphics[width=\linewidth]{linear_bids.png}
  \caption{Base bid vs. Placed bids}
  \label{fig:bids}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{linear_wins.png}
  \caption{Base bid vs. Wins}
  \label{fig:bids}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{linear_clicks.png}
  \caption{Base bid vs. Clicks}
  \label{fig:bids}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{linear_CTR.png}
  \caption{Base bid vs. CTR}
  \label{fig:bids}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{linear_CPM.png}
  \caption{Base bid vs. CPM}
  \label{fig:bids}
\end{figure}

\begin{figure}
  \includegraphics[width=\linewidth]{linear_CPC.png}
  \caption{Base bid vs. CPC}
  \label{fig:bids}
\end{figure}

\subsubsection{Linear Bidding Strategy}

In this linear model strategy, the bidding price for a given impression is proportional to its predicted CTR. Specifially, the formula can be written as $bid = C * (pCTR / avg CTR) $, where C is a constant that we'll refer to as the \textit{base bid}. Conceptually, the base bid would be the bid price for an impression with average CTR, i.e. for which the ratio $pCTR / avg CTR$ is $1$. For impressions with higher-than-average predicted CTR, the ratio would be over 1 and the base bid would be proportionally higher than the value of the base bid.

The first step was to calculate the mean CTR by averaging the value of the 'click' field in the training dataset. To decide on a good value for the constant \textit{base bid}, we varied the base bid value in the range $[0, 50]$ and ran the evaluation process numerous times. The results appear in Figures 9 through 14.

Figures 9 and 10 plot the base bid against the number of bids placed and the number of bids won respectively. The two graphs are compatible: for small base bid values, the whole budget never gets spent, s othe number of bids placed is constant and equal to the total number of impressions in the dataset, as evidence by the linear horizontal trend in Figure 9. Since the base bid is small, the corresponding range in Figure 10 shows an increasing trend in wins, as increasing the base bid leads to more percentage of the budget being spent, and thus more wins. After the threshold value of 7, where the global maximum of Figure 10 is observed,  the trend in both graphs becomes asymptotically decreasing. The whole budget is now being spent, and an increasing base bid implies more expensive - and thus less - auctions being won.

Figures 11 and 12 plot the absolute number of clicks and click-through rate percentages respectively, for the same range of base bids as figures 9 and 10. As expected, for very small base bids, no clicks are observed. Interestingly, the maximum CTR of the solution is the first non-zero data point, where $base bid = 5$. Eight conversions are achieved with the particular value, for a reasonably high CTR value of $0.263\%$, the peak of the CTR plot. The trend of figure 11 is strictly non-increasing from that point on, whereas in figure 12 several local maxima are observed. Above a base bid value of $9$, however, the CTR consistently remains under $0.1\%$.

Finally, figures 13 and 14 present money-related metrics, namely CPM and CPC as functions of the base bid. The most cost-efficient point seems to be the one with highest CTR, namely $base bid = 5$, where $CPM = 4.33$ and $CPC = 1642$. The trend in each figure is non-decreasing, which makes intuitive sense. Accoording to figures 10 and 11, both the number of wins and the number of clicks are decreasing as the base bid increases. Therefore, since the whole budget is spent, both the cost per million impressions and the average cost per click must be increasing.

For the determined optimal base bid value of $5$, we were quite satisfied with the results of our linear model. The CTR rate $0.263\%$, or more that 2 per 1000 ads being clicked, was better performance than we expected from a linear model solution. Of course, this performance constitutes a $10^3$ order of magnitude improvement over the approaches in section 4.1. It's already clear that CTR prediction through supervised learning is a huge step forward in developing successful and efficient strategies for real-time bidding.

\subsubsection{Non-linear Bidding Strategy}

The process that we used in our non-linear strategy for making CTR predictions is detailed above. The non-linear equation that we used was the ORTB1 strategy\footnote{http://wnzhang.net/papers/ortb-kdd.pdf}, which uses the following equation:
\begin{equation}\sqrt{\frac{c\theta}{\lambda}+c^2}-c\end{equation}
where $\lambda$ is the langrangian multiplier, $c$ is a constant, and $\theta$ is
the calculated pCTR value. Through trial and error, the best settings for the tunable values that I determined for my configuration were $\lambda=5*10^{-6}$ and
$c=200$. The ratio of clicked to non-clicked impressions used after our undersampling process was about 30,
and weight clicks 20 times as much in our learning process.

The results that are showed in the included table.
\begin{table}
\centering
\caption{Non-linear Strategy Results}
\begin{tabular}{|c|l|} \hline
\textbf{Statistic}&\textbf{Value}\\ \hline
Click-Through Rate&0.001140\\ \hline
Conversions&189\\ \hline
Spend&13647\\ \hline
Average CPM&82.292757\\ \hline
Average CPC&72.210370\\
\hline\end{tabular}
\end{table}
The strategy that we used could have achieved more clicks if desired, but we chose to optimise for click through rate. We only
use about half of our budget, and managed to keep average CPM and CPC fairly low in the process.

\section{Conclusion}

[TODO]

\begin{thebibliography}{4}
\bibitem{RTB}
Xuehua Shen, Jun Wang, Shuai Yuan, and Weinan Zhang.
Real-time bidding benchmarking with ipinyou
dataset.
\textit{arXiv preprint arXiv:1407.7073, 2014.}

\bibitem{second}
Gouthami Kondakindi, Vinit Parakh, Sai Kaushik Ponnekanti, Aswin Rajkumar, and Satakshi Rana.
A Logistic Regression Approach to Ad Click
Prediction.
\textit{Mach Learn Class Project, 2014}.

\bibitem{third}
Cheng Li, Yue Lu, Qiaozhu Mei, Dong Wang, and Sandeep Pandey.
Click-through Prediction for Advertising in Twitter Timeline.
\textit{University of Michigan, KDD 2015 Sydney}.

\bibitem{fourth}
H. Brendan McMahan, Gary Holt, D. Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, Sharat Chikkerur, Dan Liu, Martin Wattenberg, Arnar Mar Hrafnkelsson, Tom Boulos, and Jeremy Kubica.
Ad Click Prediction: a View from the Trenches.
\textit{Google, Inc. KDD 2013 Chicago}.

\bibitem{ORTB}
W. Zhang, S. Yuan, and J. Wang.
Optimal real-timebidding for display advertising.
Real-time bidding benchmarking with ipinyou, 2014.

\end{thebibliography}

\end{document}
